#### general settings
name: 001_MSRResNetx4_scratch_ICVL
distortion: sr
scale: 4

pretrain_model_T: 30000_G.pth

# logger options
image_save_iter: 100          # How often do you want to save output images during training
image_display_iter: 100       # How often do you want to display output images during training
display_size: 16              # How many images do you want to display each time
snapshot_save_iter: 10000     # How often do you want to save trained models
snapshot_save_epoch: 1        # How often of epochs do you want to save trained models
eval_interval: 1              # How often do you want to eval the model
log_iter: 100                 # How often do you want to show the training information

# optimization options
n_epoch: 500                  # number of training epochs
batch_size: 1                 # batch size
weight_decay: 0.0001          # weight decay
beta1: 0.9                    # Adam parameter
beta2: 0.999                  # Adam parameter
init: kaiming                 # initialization [gaussian/kaiming/xavier/orthogonal]
lr: 0.0002                    # initial learning rate
lr_policy: step               # learning rate scheduler
step_size: 100000             # how often to decay learning rate
gamma: 0.5                    # how much to decay learning rate
vgg_w: 10.                    # weight of perceptual loss
gan_w: 0.01                   # weight of GAN loss, default is 0.01
grad_w: 1.0                   # weight of gradient loss in discriminator
pixel_w: 1.0                  # weight of pixel loss, i.e., L1, default is 1.0
retina_w: 100.0               # weight of retina loss, i.e., Gradient loss, default is 1.0
pairwise_scale: 0.5

# model options
gen:
  dim: 64                     # number of filters in the bottommost layer
  mlp_dim: 256                # number of filters in MLP
  style_dim: 8                # length of style code
  n_layer: 3                  # number of layers in feature merger/splitor
  activ: relu                 # activation function [relu/lrelu/prelu/selu/tanh]
  n_downsample: 2             # number of downsampling layers in content encoder
  n_res: 4                    # number of residual blocks in content encoder/decoder
  pad_type: reflect           # padding type [zero/reflect]
  norm: in                    # norm type [in/bn/none]
  encoder_name: vgg19         # encoder name [vgg11/vgg19]
  vgg_pretrained: 1           # whether to use pretrained vgg [0/1]
  decoder_mode: Residual      # decoder mode: [Basic/Residual]
dis:
  dim: 64                     # number of filters in the bottommost layer
  norm: none                  # normalization layer [none/bn/in/ln]
  activ: lrelu                # activation function [relu/lrelu/prelu/selu/tanh]
  n_layer: 4                  # number of layers in D
  gan_type: lsgan             # GAN loss [lsgan/nsgan]
  num_scales: 3               # number of scales
  pad_type: reflect           # padding type [zero/reflect]
  use_grad: 1                 # whether use gradient constraint[0/1]
  use_wasserstein: 0          # is use wasserstein distance loss [0/1]

#### network structures
network_T:
  in_nc: 31
  out_nc: 31
  nf: 64
  nb: 16
  upscale: 4

network_S:
  in_nc: 31
  out_nc: 31
  nf: 64
  nb: 16
  upscale: 4

#### datasets
datasets:
  train:
    name: ICVL151
    mode: LQGT
    dataroot_GT: ../MMSR/datasets/train_data/ICVL151_sub
    dataroot_LQ: ../MMSR/datasets/train_data/ICVL151_sub_bicLRx4

    use_shuffle: true
    n_workers: 0  # per GPU
    batch_size: 16
    GT_size: 128
    use_flip: true
    use_rot: true
  val:
    name: val
    mode: LQGT
    dataroot_GT: ../MMSR/datasets/valid/test_mat
    dataroot_LQ: ../MMSR/datasets/valid/test_mat_bicLRx4